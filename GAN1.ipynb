{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "# ========================\n",
    "# 1. Load the MNIST Dataset\n",
    "# ========================\n",
    "\n",
    "# Load the dataset\n",
    "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data: Normalize to [-1, 1]\n",
    "x_train = x_train.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "x_train = x_train * 2 - 1  # Normalize to [-1, 1]\n",
    "x_train = x_train.reshape(-1, 784)  # Flatten images\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Create a tf.data.Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# ========================\n",
    "# 2. Define the Generator Network\n",
    "# ========================\n",
    "\n",
    "def build_generator(input_size=100, output_size=784):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, input_dim=input_size),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(512),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(output_size, activation='tanh'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ========================\n",
    "# 3. Define the Discriminator Network\n",
    "# ========================\n",
    "\n",
    "def build_discriminator(input_size=784):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512, input_dim=input_size),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ========================\n",
    "# 4. Initialize Networks, Loss Function, and Optimizers\n",
    "# ========================\n",
    "\n",
    "# Instantiate the networks\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Loss function\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs('generated_images', exist_ok=True)\n",
    "\n",
    "# Fixed noise for consistent image generation\n",
    "fixed_noise = tf.random.normal([64, 100])\n",
    "\n",
    "# ========================\n",
    "# 5. Training Loop\n",
    "# ========================\n",
    "\n",
    "num_epochs = 100\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []\n",
    "epochs_to_save = [1, 10, 50, 100]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for real_images in train_dataset:\n",
    "        batch_size = real_images.shape[0]\n",
    "        real_images = tf.reshape(real_images, [batch_size, -1])\n",
    "        real_labels = tf.ones((batch_size, 1))\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Generate noise\n",
    "        noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Discriminator\n",
    "        # ---------------------\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # Generate fake images\n",
    "            fake_images = generator(noise, training=True)\n",
    "\n",
    "            # Discriminator outputs\n",
    "            real_output = discriminator(real_images, training=True)\n",
    "            fake_output = discriminator(fake_images, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            d_loss_real = cross_entropy(real_labels, real_output)\n",
    "            d_loss_fake = cross_entropy(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # Compute gradients and update discriminator\n",
    "        gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "        # ---------------------\n",
    "        # Train Generator\n",
    "        # ---------------------\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # Generate fake images\n",
    "            fake_images = generator(noise, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            fake_output = discriminator(fake_images, training=True)\n",
    "\n",
    "            # Generator wants discriminator to believe generated images are real\n",
    "            g_loss = cross_entropy(real_labels, fake_output)\n",
    "\n",
    "        # Compute gradients and update generator\n",
    "        gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    # Record losses\n",
    "    G_losses.append(g_loss.numpy())\n",
    "    D_losses.append(d_loss.numpy())\n",
    "\n",
    "    # Save generated images at specified epochs\n",
    "    if epoch in epochs_to_save:\n",
    "        # Generate images from fixed noise\n",
    "        fake_images = generator(fixed_noise, training=False)\n",
    "        fake_images = tf.reshape(fake_images, [-1, 28, 28, 1])\n",
    "        fake_images = (fake_images + 1) / 2.0  # Rescale images to [0,1]\n",
    "\n",
    "        # Create a grid of images and save\n",
    "        def save_images(images, epoch):\n",
    "            grid_size = int(np.sqrt(images.shape[0]))\n",
    "            fig, axs = plt.subplots(grid_size, grid_size, figsize=(grid_size, grid_size))\n",
    "            idx = 0\n",
    "            for i in range(grid_size):\n",
    "                for j in range(grid_size):\n",
    "                    axs[i, j].imshow(images[idx, :, :, 0], cmap='gray')\n",
    "                    axs[i, j].axis('off')\n",
    "                    idx += 1\n",
    "            plt.subplots_adjust(wspace=0, hspace=0)\n",
    "            plt.savefig(f'generated_images/generated_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "\n",
    "        save_images(fake_images.numpy(), epoch)\n",
    "        print(f'Epoch [{epoch}/{num_epochs}]  Loss D: {d_loss.numpy():.4f}, Loss G: {g_loss.numpy():.4f}')\n",
    "\n",
    "# ========================\n",
    "# 6. Plot Loss Curves\n",
    "# ========================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(range(1, num_epochs + 1), G_losses, label=\"Generator\")\n",
    "plt.plot(range(1, num_epochs + 1), D_losses, label=\"Discriminator\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('generated_images/loss_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# ========================\n",
    "# 7. Visualize Generated Images\n",
    "# ========================\n",
    "\n",
    "def show_saved_images(epochs):\n",
    "    for epoch in epochs:\n",
    "        img_path = f'generated_images/generated_epoch_{epoch}.png'\n",
    "        image = mpimg.imread(img_path)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Generated Images at Epoch {epoch}')\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "# Display the images for epochs 1, 10, 50, and 100\n",
    "show_saved_images(epochs_to_save)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
