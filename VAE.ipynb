{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: GPU Configuration\n",
    "# ---------------------------\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "\n",
    "# b. Enable Dynamic GPU Memory Allocation\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Define the VAE Components\n",
    "# ---------------------------\n",
    "\n",
    "# Sampling function using the reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "latent_dim = 10\n",
    "encoder_inputs = layers.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
    "decoder = models.Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# VAE Model with custom loss and metrics\n",
    "class VAEWithMetrics(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAEWithMetrics, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"elbo\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.keras.losses.binary_crossentropy(\n",
    "            K.flatten(inputs), K.flatten(reconstructed)\n",
    "        )\n",
    "        reconstruction_loss *= 28 * 28\n",
    "        reconstruction_loss = tf.reduce_mean(reconstruction_loss)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = tf.reduce_sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        self.add_loss(total_loss)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return reconstructed\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "# Custom callback to store losses\n",
    "class VAEHistory(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(VAEHistory, self).__init__()\n",
    "        self.elbo = []\n",
    "        self.kl_loss = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.elbo.append(logs.get('elbo'))\n",
    "        self.kl_loss.append(logs.get('kl_loss'))\n",
    "\n",
    "# Initialize and compile VAE\n",
    "vae = VAEWithMetrics(encoder, decoder)\n",
    "learning_rate = 1e-3\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "vae.compile(optimizer=optimizer)\n",
    "\n",
    "# Initialize callback\n",
    "history_callback = VAEHistory()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Train the VAE\n",
    "# ---------------------------\n",
    "\n",
    "# Reduce batch size to prevent OOM\n",
    "history = vae.fit(\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,  # Reduced from 128\n",
    "    validation_data=(x_test, None),\n",
    "    callbacks=[history_callback]\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Plot ELBO and KL-Divergence Separately\n",
    "# ---------------------------\n",
    "\n",
    "# Plot ELBO Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_callback.elbo, label='ELBO Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('ELBO Loss')\n",
    "plt.title('ELBO Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot KL-Divergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_callback.kl_loss, label='KL Divergence', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.title('KL Divergence Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 6: Visualize Reconstructions\n",
    "# ---------------------------\n",
    "\n",
    "def plot_reconstructions(model, data, n=10):\n",
    "    z_mean, z_log_var, z = model.encoder.predict(data)\n",
    "    reconstructed = model.decoder.predict(z)\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(data[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstructed\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot reconstructions\n",
    "plot_reconstructions(vae, x_test)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 7: Generate New Images\n",
    "# ---------------------------\n",
    "\n",
    "def plot_generated_images(decoder, latent_dim=10, num_images=10):\n",
    "\n",
    "    # Sample from standard normal distribution\n",
    "    z_samples = np.random.normal(size=(num_images, latent_dim))\n",
    "    \n",
    "    # Generate images using the decoder\n",
    "    generated = decoder.predict(z_samples)\n",
    "    \n",
    "    # Plot the generated images\n",
    "    plt.figure(figsize=(num_images * 2, 2))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(generated[i].squeeze(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "# Plot generated images\n",
    "plot_generated_images(decoder, latent_dim=latent_dim, num_images=15)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 8: VAE Latent Space Visualization\n",
    "# ---------------------------\n",
    "\n",
    "# Encode the test data\n",
    "z_mean, z_log_var, z = encoder.predict(x_test, batch_size=64)\n",
    "\n",
    "# Load test labels for coloring\n",
    "(_, y_train_labels), (_, y_test_labels) = mnist.load_data()\n",
    "y_test_labels = y_test_labels.astype('int')\n",
    "\n",
    "# Create a scatter plot of the latent space\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(z[:, 0], z[:, 1], c=y_test_labels, cmap='tab10', alpha=0.5)\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.xlabel('z[0]')\n",
    "plt.ylabel('z[1]')\n",
    "plt.title('VAE Latent Space Visualization')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 9: VAE for Anomaly Detection\n",
    "# ---------------------------\n",
    "\n",
    "# Function to add noise to create anomalous data\n",
    "def add_noise(data, noise_factor=0.5):\n",
    "    noisy = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
    "    noisy = np.clip(noisy, 0., 1.)\n",
    "    return noisy\n",
    "\n",
    "# Create noisy test data as anomalies\n",
    "x_test_noisy = add_noise(x_test.copy(), noise_factor=0.5)\n",
    "\n",
    "# Compute Reconstruction Errors\n",
    "def compute_reconstruction_errors(model, data):\n",
    "    reconstructed = model.predict(data)\n",
    "    errors = np.mean(np.square(data - reconstructed), axis=(1,2,3))\n",
    "    return errors\n",
    "\n",
    "# Compute errors for normal and anomalous data\n",
    "errors_normal = compute_reconstruction_errors(vae, x_test)\n",
    "errors_anomalous = compute_reconstruction_errors(vae, x_test_noisy)\n",
    "\n",
    "# Plot the Distribution of Reconstruction Errors\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(errors_normal, bins=50, alpha=0.6, label='Normal', color='green', density=True)\n",
    "plt.hist(errors_anomalous, bins=50, alpha=0.6, label='Anomalous', color='red', density=True)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Step 10: Set Threshold and Classify Anomalies\n",
    "# ---------------------------\n",
    "\n",
    "# Set threshold based on the 95th percentile of normal reconstruction errors\n",
    "threshold = np.percentile(errors_normal, 95)\n",
    "print(f\"Reconstruction error threshold: {threshold}\")\n",
    "\n",
    "# Classify\n",
    "y_pred_normal = errors_normal < threshold\n",
    "y_pred_anomalous = errors_anomalous < threshold\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create labels: 0 for normal, 1 for anomalous\n",
    "y_true = np.concatenate([np.zeros_like(errors_normal), np.ones_like(errors_anomalous)])\n",
    "y_pred = np.concatenate([y_pred_normal, y_pred_anomalous])\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomalous']))\n",
    "\n",
    "# ---------------------------\n",
    "# Step 11: ROC PLOT\n",
    "# ---------------------------\n",
    "\n",
    "# Create a combined array of all errors\n",
    "errors_all = np.concatenate([errors_normal, errors_anomalous])\n",
    "\n",
    "# Create y_true where 0 denotes normal and 1 denotes anomalous\n",
    "y_true = np.concatenate([np.zeros(len(errors_normal)), np.ones(len(errors_anomalous))])\n",
    "\n",
    "# Compute ROC curve and AUC using reconstruction errors as scores\n",
    "fpr, tpr, thresholds = roc_curve(y_true, errors_all)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
